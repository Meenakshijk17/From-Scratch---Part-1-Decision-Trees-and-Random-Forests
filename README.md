# From Scratch - Part-1: Decision Trees and Random Forests

### Aim
This project aims to develop and implement decision trees and random forests from scratch, without using ML libraries like Scikit-Learn.


### Motivation - More than just a *model.fit()*
Implementing decision tree and random forest algorithms from scratch is crucial for deepening one's understanding of these machine learning techniques. By building these algorithms manually, you gain insights into their inner workings and intricacies. This hands-on approach helps solidify concepts such as recursive partitioning, feature importance, and ensemble learning. Through implementation, you can customize the algorithms, experiment with different splitting criteria, handle categorical variables, and fine-tune hyperparameters. Debugging and optimizing these implementations also offer practical insights into tackling challenges like overfitting and improving model performance. Moreover, crafting these algorithms from the ground up fosters a deeper appreciation for the theoretical foundations behind decision trees and random forests, enhancing comprehension beyond what's possible through mere usage of pre-built libraries. This exercise encourages critical thinking and empowers practitioners to tailor algorithms to specific use cases, ultimately advancing proficiency in machine learning and data science.
